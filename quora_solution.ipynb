{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start \n",
      "lstm_units: 193\n",
      "dense_units: 136\n",
      "lstm_dropout: 0.19\n",
      "dense_dropout: 0.18\n",
      "leaks_dense_units: 68\n",
      "optimizer: nadam\n",
      "lr: 0.001\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# 导入要用到的类\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from string import punctuation\n",
    "from functools import partial\n",
    "\n",
    "np.random.seed(110)\n",
    "random.seed(110)\n",
    "\n",
    "lstm_units = 193\n",
    "dense_units = 136\n",
    "lstm_dropout = 0.19\n",
    "dense_dropout = 0.18\n",
    "leaks_dense_units = 68\n",
    "optimizer = 'nadam'\n",
    "lr = 0.001\n",
    "d_train_dir = '.'\n",
    "d_result_dir = '.'\n",
    "\n",
    "EMBEDDING_FILE = d_train_dir + '/glove.840B.300d.txt'\n",
    "TRAIN_DATA_FILE = d_train_dir + '/train.csv'\n",
    "TEST_DATA_FILE = d_train_dir + '/test.csv'\n",
    "MAX_SEQ_LENGTH = 30\n",
    "MAX_NUMBER_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "print('start ')\n",
    "print(\"lstm_units:\", lstm_units)\n",
    "print(\"dense_units:\", dense_units)\n",
    "print(\"lstm_dropout:\", lstm_dropout)\n",
    "print(\"dense_dropout:\", dense_dropout)\n",
    "print(\"leaks_dense_units:\", leaks_dense_units)\n",
    "print(\"optimizer:\", optimizer)\n",
    "print(\"lr:\", lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignore error could not convert string to float: '.'\n",
      "ignore error could not convert string to float: 'name@domain.com'\n",
      "ignore error could not convert string to float: '.'\n",
      "ignore error could not convert string to float: 'name@domain.com'\n",
      "ignore error could not convert string to float: '.'\n",
      "ignore error could not convert string to float: '.'\n",
      "ignore error could not convert string to float: 'name@domain.com'\n",
      "ignore error could not convert string to float: 'name@domain.com'\n",
      "ignore error could not convert string to float: 'name@domain.com'\n",
      "ignore error could not convert string to float: 'name@domain.com'\n",
      "ignore error could not convert string to float: 'name@domain.com'\n",
      "ignore error could not convert string to float: 'Killerseats.com'\n",
      "ignore error could not convert string to float: 'name@domain.com'\n",
      "ignore error could not convert string to float: 'mylot.com'\n",
      "ignore error could not convert string to float: 'name@domain.com'\n",
      "ignore error could not convert string to float: 'name@domain.com'\n",
      "ignore error could not convert string to float: 'name@domain.com'\n",
      "ignore error could not convert string to float: 'name@domain.com'\n",
      "ignore error could not convert string to float: 'Amazon.com'\n",
      "ignore error could not convert string to float: 'name@domain.com'\n",
      "word vectors in glove: 2195884\n"
     ]
    }
   ],
   "source": [
    "# 初始化语料库\n",
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE, 'r', encoding=\"utf-8\") as f:\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except ValueError as ve:\n",
    "            print('ignore error ' + str(ve))\n",
    "\n",
    "print('word vectors in glove: %d' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据清洗方法\n",
    "stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "\n",
    "\n",
    "# 数据清洗参考 https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
    "def text_to_wordlist(text, remove_stop_words=True):\n",
    "\n",
    "    text = text.lower()\\\n",
    "                .replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\") \\\n",
    "                .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\") \\\n",
    "                .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\") \\\n",
    "                .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\") \\\n",
    "                .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\") \\\n",
    "                .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \") \\\n",
    "                .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"=\", \" equal \").replace(\"+\", \" plus \")\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', text)\n",
    "    text = re.sub(r\"([0-9]+)000000\", r\"\\1m\", text)\n",
    "    text = re.sub(r\"([0-9]+)000\", r\"\\1k\", text)\n",
    "\n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "\n",
    "    # Return a list of words\n",
    "    return text\n",
    "\n",
    "parse_apply_func = partial(text_to_wordlist, remove_stop_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.csv length:404267\n",
      "test.csv length:2345796\n"
     ]
    }
   ],
   "source": [
    "# 数据清洗\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "train_df[\"question1\"] = train_df[\"question1\"].fillna(\"\").apply(parse_apply_func)\n",
    "train_df[\"question2\"] = train_df[\"question2\"].fillna(\"\").apply(parse_apply_func)\n",
    "texts_1 = train_df[\"question1\"].tolist()\n",
    "texts_2 = train_df[\"question2\"].tolist()\n",
    "labels = train_df[\"is_duplicate\"].tolist()\n",
    "print('train.csv length:%d' % len(texts_1))\n",
    "\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)\n",
    "test_df[\"question1\"] = test_df[\"question1\"].fillna(\"\").apply(parse_apply_func)\n",
    "test_df[\"question2\"] = test_df[\"question2\"].fillna(\"\").apply(parse_apply_func)\n",
    "test_texts_1 = test_df[\"question1\"].tolist()\n",
    "test_texts_2 = test_df[\"question2\"].tolist()\n",
    "test_ids = test_df[\"test_id\"].tolist()\n",
    "print('test.csv length:%d' % len(test_texts_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 120676\n",
      "data_1.shape: (404267, 30)\n",
      "labels.shape: (404267,)\n"
     ]
    }
   ],
   "source": [
    "# 生成主要特征\n",
    "tokenizer = Tokenizer(num_words=MAX_NUMBER_WORDS)\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('tokens:', str(len(word_index)))\n",
    "\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQ_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQ_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('data_1.shape:', data_1.shape)\n",
    "print('labels.shape:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQ_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQ_LENGTH)\n",
    "test_ids = np.array(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/test/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/zeus/test/lib/python3.6/site-packages/ipykernel_launcher.py:38: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "/home/zeus/test/lib/python3.6/site-packages/ipykernel_launcher.py:39: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "# 生成弱特征\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "mix_df = pd.concat([train_df[['question1', 'question2']],\n",
    "                    test_df[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(mix_df.shape[0]):\n",
    "    q_dict[mix_df.question1[i]].add(mix_df.question2[i])\n",
    "    q_dict[mix_df.question2[i]].add(mix_df.question1[i])\n",
    "\n",
    "\n",
    "def q1_freq(row):\n",
    "    return len(q_dict[row['question1']])\n",
    "\n",
    "\n",
    "def q2_freq(row):\n",
    "    return len(q_dict[row['question2']])\n",
    "\n",
    "\n",
    "def q1_q2_intersect(row):\n",
    "    return len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']])))\n",
    "\n",
    "\n",
    "train_df['q1_q2_intersect'] = train_df.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "train_df['q1_freq'] = train_df.apply(q1_freq, axis=1, raw=True)\n",
    "train_df['q2_freq'] = train_df.apply(q2_freq, axis=1, raw=True)\n",
    "\n",
    "test_df['q1_q2_intersect'] = test_df.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "test_df['q1_freq'] = test_df.apply(q1_freq, axis=1, raw=True)\n",
    "test_df['q2_freq'] = test_df.apply(q2_freq, axis=1, raw=True)\n",
    "\n",
    "leaks = train_df[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\n",
    "test_leaks = test_df[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((leaks, test_leaks)))\n",
    "leaks = ss.transform(leaks)\n",
    "test_leaks = ss.transform(test_leaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 33413\n"
     ]
    }
   ],
   "source": [
    "# 生成嵌入层\n",
    "nb_words = min(MAX_NUMBER_WORDS, len(word_index)) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切分数据\n",
    "index_array = [*range(len(data_1))]\n",
    "random.shuffle(index_array)\n",
    "idx_train = index_array[:int(len(data_1) * (1 - VALIDATION_SPLIT))]\n",
    "idx_val = index_array[int(len(data_1) * (1 - VALIDATION_SPLIT)):]\n",
    "\n",
    "# 这里需要将data_1和data_2，双向stack一下，为了解决数据对称性问题\n",
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "leaks_train = np.vstack((leaks[idx_train], leaks[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "leaks_val = np.vstack((leaks[idx_val], leaks[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "标签重新分配权重\n",
    "这里的值参考\n",
    "https://github.com/howardyclo/Kaggle-Quora-Question-Pairs#class-label-reweighting\n",
    "0.174/0.369 = 0.471544715\n",
    "(1-0.174/(1-0.369) = 1.30903328\n",
    "'''\n",
    "validation_weight = np.ones(len(labels_val))\n",
    "validation_weight *= 0.471544715\n",
    "validation_weight[labels_val == 0] = 1.30903328\n",
    "model_class_weight = {0: 1.30903328, 1: 0.471544715}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 30, 300)      36203100    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 193)          381368      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 68)           272         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 454)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 454)          1816        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 454)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 136)          61880       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 136)          544         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 136)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            137         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 36,649,117\n",
      "Trainable params: 444,837\n",
      "Non-trainable params: 36,204,280\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 构建神经网络\n",
    "embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQ_LENGTH,\n",
    "                            trainable=False)\n",
    "lstm_layer = LSTM(lstm_units, dropout=lstm_dropout, recurrent_dropout=lstm_dropout)\n",
    "\n",
    "sequence_input1 = Input(shape=(MAX_SEQ_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_input1)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_input2 = Input(shape=(MAX_SEQ_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_input2)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "leaks_input = Input(shape=(leaks.shape[1],))\n",
    "leaks_dense = Dense(leaks_dense_units, activation='relu')(leaks_input)\n",
    "\n",
    "merged = concatenate([x1, y1, leaks_dense])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(dense_dropout)(merged)\n",
    "\n",
    "merged = Dense(dense_units, activation='relu')(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(dense_dropout)(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "my_optimizer = optimizers.Adam(lr=lr)\n",
    "if optimizer == 'sgd':\n",
    "    my_optimizer = optimizers.SGD(lr=lr)\n",
    "elif optimizer == 'nadam':\n",
    "    my_optimizer = optimizers.Nadam(lr=lr)\n",
    "\n",
    "model = Model(inputs=[sequence_input1, sequence_input2, leaks_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer=my_optimizer, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 727680 samples, validate on 80854 samples\n",
      "Epoch 1/200\n",
      " - 156s - loss: 0.2917 - acc: 0.8189 - val_loss: 0.2400 - val_acc: 0.8359\n",
      "Epoch 2/200\n",
      " - 153s - loss: 0.2402 - acc: 0.8359 - val_loss: 0.2272 - val_acc: 0.8417\n",
      "Epoch 3/200\n",
      " - 153s - loss: 0.2271 - acc: 0.8404 - val_loss: 0.2187 - val_acc: 0.8491\n",
      "Epoch 4/200\n",
      " - 153s - loss: 0.2173 - acc: 0.8452 - val_loss: 0.2135 - val_acc: 0.8566\n",
      "Epoch 5/200\n",
      " - 153s - loss: 0.2089 - acc: 0.8501 - val_loss: 0.2037 - val_acc: 0.8585\n",
      "Epoch 6/200\n",
      " - 153s - loss: 0.2018 - acc: 0.8546 - val_loss: 0.1997 - val_acc: 0.8605\n",
      "Epoch 7/200\n",
      " - 153s - loss: 0.1955 - acc: 0.8591 - val_loss: 0.1972 - val_acc: 0.8681\n",
      "Epoch 8/200\n",
      " - 153s - loss: 0.1892 - acc: 0.8633 - val_loss: 0.2087 - val_acc: 0.8781\n",
      "Epoch 9/200\n",
      " - 153s - loss: 0.1823 - acc: 0.8667 - val_loss: 0.1890 - val_acc: 0.8766\n",
      "Epoch 10/200\n",
      " - 153s - loss: 0.1810 - acc: 0.8705 - val_loss: 0.1858 - val_acc: 0.8837\n",
      "Epoch 11/200\n",
      " - 153s - loss: 0.1770 - acc: 0.8733 - val_loss: 0.1971 - val_acc: 0.8784\n",
      "Epoch 12/200\n",
      " - 153s - loss: 0.1734 - acc: 0.8761 - val_loss: 0.2033 - val_acc: 0.8780\n",
      "Epoch 13/200\n",
      " - 153s - loss: 0.1703 - acc: 0.8789 - val_loss: 0.1915 - val_acc: 0.8778\n",
      "best_val_score:0.185756\n"
     ]
    }
   ],
   "source": [
    "#开始训练\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "best_model_path = d_result_dir + '/best_model.h5'\n",
    "model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "hist = model.fit([data_1_train, data_2_train, leaks_train], labels_train,\n",
    "                 validation_data=([data_1_val, data_2_val, leaks_val], labels_val, validation_weight),\n",
    "                 epochs=200, batch_size=4096, shuffle=True,\n",
    "                 callbacks=[early_stopping, model_checkpoint],\n",
    "                 class_weight=model_class_weight, verbose=2)\n",
    "\n",
    "model.load_weights(best_model_path)\n",
    "best_val_score = min(hist.history['val_loss'])\n",
    "print(\"best_val_score:%f\" % best_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于输入层含有双向特征，这里需要Fine tune一下\n",
    "preds = model.predict([test_data_1, test_data_2, test_leaks], batch_size=8192)\n",
    "preds += model.predict([test_data_2, test_data_1, test_leaks], batch_size=8192)\n",
    "preds /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存结果\n",
    "submission = pd.DataFrame({'test_id': test_ids, 'is_duplicate': preds.ravel()})\n",
    "submission.to_csv(d_result_dir + '/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
